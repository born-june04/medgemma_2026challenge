use_stub_encoders: false
audio_checkpoint: "google/hear-pytorch"
# MedSigLIP model id (update if you want a different checkpoint)
image_checkpoint: "google/medsiglip-448"
llm_adapter: placeholder
hf_token: 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

# --- Audio classifier head (new) ---
audio_classifier:
  enabled: true                          # When true, pipeline can run classifier inference if ckpt exists
  checkpoint_path: artifacts/audio_classifier_head.pt
  balanced_sampling: true

  # Training is a separate, explicit step. Keep this false unless your training script wants to use it.
  auto_train_if_missing: false

  # IMPORTANT: Set this to the 'dataset' directory level:
  #   <dataset_root>/
  #     <class_A>/CSI/*.png
  #     <class_B>/CSI/*.png
  dataset_root: "data/Chest_Diseases_Dataset"

  # Where to store the cached embeddings from the frozen encoder (for fast training)
  cache_path: artifacts/audio_embeddings_cache.pt

  # Head architecture: null => Linear head; set an integer (e.g., 256) for a tiny MLP
  hidden_dim: 768 ## 768 -> best acc = 0.875
  dropout: 0.0

  # Training hyperparameters (used only by the training step)
  batch_size: 36
  lr: 0.001
  weight_decay: 0.0001
  max_epochs: 1024
  val_ratio: 0.2
  early_stopping_patience: 1024
  amp: False

# --- Image classifier head (CXR) ---
image_classifier:
  enabled: true
  checkpoint_path: artifacts/image_classifier_head.pt
  balanced_sampling: true
  dataset_root: "data/Chest_Diseases_Dataset"
  cache_path: artifacts/image_embeddings_cache.pt
  hidden_dim: 768
  dropout: 0.0
  batch_size: 36
  lr: 0.001
  weight_decay: 0.0001
  max_epochs: 1024
  val_ratio: 0.2
  early_stopping_patience: 1024
  amp: False
